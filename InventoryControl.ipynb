{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inventory Control under Non-Stationary Demand\n",
    "Optimal inventory control is an active field of research in management science and a classic problem of stochastic optimization. The problem extends the newsvendor model as it allows to store unsold items for sale in a subsequent selling period.\n",
    "\n",
    "## The Optimization Problem\n",
    "|Symbol|Description|\n",
    "|---|---|\n",
    "|$t$|time index|\n",
    "|$s_t$|sell|\n",
    "|$b_t$|buy|\n",
    "|$i_t$|inventory with $i_0$ fixed|\n",
    "|$d_t$|random demand|\n",
    "|$D_t$|demand history up to $t$|\n",
    "\n",
    "The decision-maker seeks to find an optimal policy that maximizes the expected rewards from buying and selling a product under uncertain demand. It is assumed that demand must be fulfilled with inventory that is on stock at the beginning of a period and that left-over inventores can be stored. A mathematical model that describes this decision problem as a so-called multistage stochastic optimization problem would look as follows:\n",
    "$$\\begin{align}\n",
    "\\max \\ & \\sum_{t=1}^T \\mathbb{E}\\left[ p s_t(D_t) - c b_t(D_t) - h i_t(D_t)\\right]& \\\\\n",
    "s.t. \\  & i_t(D_t) = i_{t-1}(D_t) - s_t(D_t) + b_t(D_t)& t=1,...,T \\\\\n",
    "& s_t(D_t) \\leq i_{t-1}(D_t)& t=1,...,T \\\\\n",
    "& s_t(D_t) \\leq d_t& t=1,...,T. \n",
    "\\end{align}$$\n",
    "\n",
    "We now use QUASAR stochastic optimizer to model, solve, and analyze this optimization problem. The optimizer can be accessed through a Python library which must be imported first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyquasar import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Decision Problem\n",
    "First, we must first define a couple of parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_months = 12\n",
    "initial_inventory = 12\n",
    "sales_price = 5\n",
    "inventory_cost = 0.5\n",
    "buying_price = 2\n",
    "max_inventory = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we make the same type of decisions in each period, we add the same variables and equations during each loop. At the end of a loop, we change the reference of <code>begin_inventory</code> to <code>end_inventory</code>. Python is very flexible with the type of the variable <code>begin_inventory</code> which was an <code>int</code> during the first loop and became a <code>DecisionVariable</code> afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = DecisionProblem()\n",
    "begin_inventory = initial_inventory\n",
    "for t in range(num_months):\n",
    "    \n",
    "    # decision variables\n",
    "    buy, sell, end_inventory = model.add_variables(t, \"buy\", \"sell\", \"inventory\")\n",
    "    \n",
    "    # bounds\n",
    "    model += sell <= rand(\"demand\")\n",
    "    model += sell <= begin_inventory\n",
    "    model += end_inventory <= max_inventory\n",
    "\n",
    "    # inventory balance\n",
    "    model += end_inventory == begin_inventory - sell + buy\n",
    "\n",
    "    # objective function / immediate reward\n",
    "    model += sales_price*sell - inventory_cost*end_inventory - buying_price*buy\n",
    "    \n",
    "    # state transition\n",
    "    begin_inventory = end_inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Remark\n",
    "Since inventory is a time-coupling variable, it is recommended to add a very loose upper bound for this variable so that the model fulfills the relatively complete recourse assumption. This may seem rather theoretical, but it often happens that the solution algorithm is  if the equivalent dynamic programming formulation becomes unbounded.\n",
    "\n",
    "### The Stochastic Demand Process\n",
    "Often, we do not know where our sales are going, except they they are somewhat tied to some macroeconomic factors that are hard to predict and can eventually end up trending in any direction.\n",
    "\n",
    "Instead of putting too many assumptions into our demand model, let's just say that there is a relative relationship between the currently observed demand and next period's demand. This ensures that demand is positive and demand shocks increase with the current demand level. If we assume that the log-ratio of these shocks follows a normal distribution and that there is no drift, we arrive at the geometric Brownian motion as our stochastic demand model. The GBM demand process is defined as\n",
    "$$d_t = \\exp \\left( \\log(d_t) + \\varepsilon \\right), \\ \\varepsilon \\sim N(0,\\sigma).$$\n",
    "\n",
    "To define the demand process, we have to define the initial state of the demand process, namely the demand of this months, as well as the standard deviation of the log-normal ditribution which defines the distribution of percentage change from one period to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name =\"demand\"\n",
    "drift = 0\n",
    "sigma = 0.2\n",
    "initial_demand = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In QUASAR, we can create a GBM process quite easily by using the <code>GBM</code> class. Once the object has been created, a simulation sample can be drawn, analyzed, and visualized in various ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbm_demand = GBM(name, volatility=sigma, drift = 0, initial_state=initial_demand)\n",
    "sample = gbm_demand.simulate(num_stages=num_months, sample_size=100)\n",
    "sample.demand.spaghetti(title='Monthly Demand as Geometric Brownian Motion', ylabel='Quantity', xlabel='Month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Method\n",
    "To solve the stochastic decision problem to optimality we will use QUASAR's dynamic optimizer. The dynamic optimizer automatically computes an approximation of the optimal decision policy. Internally, it approximates the stochastic process by using a small set of discrete price states called nodes. The quality of the approximation largely depends on how many of these nodes there are and how they are being selected. The optimizer handles these steps automatically. However, it is always a good idea to manually adjust the number of nodes to see if the quality of the approximation is changing. In most cases, the default setting will be sufficient.\n",
    "\n",
    "The optimizer is started by calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opt = DynamicOptimizer(model, gbm_demand, num_nodes=50)\n",
    "opt.solve()\n",
    "opt.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the optimizer has converged, we can gather some information about the solution process. Solution statistics are stored in a Pandas DataFrame which can be accessed through the attribute <code>DynamicOptimizer.stats</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expReward</th>\n",
       "      <th>simReward</th>\n",
       "      <th>stdError</th>\n",
       "      <th>sampleSize</th>\n",
       "      <th>hyperplanes</th>\n",
       "      <th>numSolves</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iter</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>341.92</td>\n",
       "      <td>380.03</td>\n",
       "      <td>33.48</td>\n",
       "      <td>20</td>\n",
       "      <td>17918</td>\n",
       "      <td>81143</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>341.79</td>\n",
       "      <td>363.05</td>\n",
       "      <td>28.45</td>\n",
       "      <td>20</td>\n",
       "      <td>17905</td>\n",
       "      <td>81695</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>341.62</td>\n",
       "      <td>343.06</td>\n",
       "      <td>21.96</td>\n",
       "      <td>20</td>\n",
       "      <td>17822</td>\n",
       "      <td>82247</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>341.21</td>\n",
       "      <td>347.87</td>\n",
       "      <td>22.08</td>\n",
       "      <td>20</td>\n",
       "      <td>18007</td>\n",
       "      <td>82799</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>340.76</td>\n",
       "      <td>337.76</td>\n",
       "      <td>6.11</td>\n",
       "      <td>400</td>\n",
       "      <td>18175</td>\n",
       "      <td>83351</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      expReward  simReward  stdError  sampleSize  hyperplanes  numSolves  duration\n",
       "iter                                                                              \n",
       "147      341.92     380.03     33.48          20        17918      81143        30\n",
       "148      341.79     363.05     28.45          20        17905      81695        30\n",
       "149      341.62     343.06     21.96          20        17822      82247        30\n",
       "150      341.21     347.87     22.08          20        18007      82799        30\n",
       "151      340.76     337.76      6.11         400        18175      83351        32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.stats.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Output Analysis\n",
    "\n",
    "Once the optimizer is finished, we can obtain the solution which takes the form of a policy, <code>opt.policy</code>. This policy can be simulated, much like the stochastic process has been simulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "solution = opt.policy.simulate(sample_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the total profit distribution by grouping the immediate rewards at each stage by their series and the forming the sum over a single series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "solution.rewards.groupby_series.sum().describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code> solution </code> holds all simulated sample paths of the optimal policy as a Pandas DataFrame, containing the values of each decision variable as well as the realizations of the random variable, but also the reward at each stage and the shadow prices of the time-coupling variables which can be used for some advanced decision analyses. \n",
    "\n",
    "Much like with the demand process, the time series of the decision variables or immediate reward can also be visualized. We can also take a look at a histogram of the total rewards distribution. Or generate a spaghetti plot of the discretized demand process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2, 2, figsize=(16,10))\n",
    "solution.rewards.groupby_series.sum().hist(ax=ax1, bins=50)\n",
    "ax1.set_title('Total Rewards')\n",
    "solution.rewards.fanchart(title='Immediate Rewards', ylabel='Profit', xlabel='', ax=ax2)\n",
    "solution.decision.inventory.fanchart(title='Inventory Levels', ylabel='Quantity', xlabel='Month', ax=ax3)\n",
    "solution.state.demand.spaghetti(title='Demands', ylabel='Quantity', xlabel='Month', alpha=.03, ax=ax4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving Simple Rules\n",
    "With the demand process being GBM and no fixed order cost given, we can condlude that the optimal control rule of an infinite horizon problem must be of type *(s,s-1)*, whereby *s* depends on the state, which in this case is demand.\n",
    "\n",
    "Let us take a look at the relationship of demand and inventory level at the end of each stage. Since the decision problem has got a finite horizon, there will be boundary effects, which may lead to biased results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unbiased_solution = solution.query('stage > 2 & stage < 9')\n",
    "f, (ax1,ax2) = plt.subplots(1, 2, figsize=(12,5))\n",
    "ax1.scatter(x=unbiased_solution.state.demand, y=unbiased_solution.decision.inventory)\n",
    "ax1.set_title('Demand vs Inventory')\n",
    "ax2.scatter(x=np.log(unbiased_solution.state.demand), y=np.log(unbiased_solution.decision.inventory))\n",
    "ax2.set_title('Log Demand vs Log Inventory')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the visual inspection, there appears to be an approximately linear relationship. Let us exploit this insight to derive an inventory control where *s* is a linear function of demand which would provide us with a state-dependent *(s,s-1)* rule.\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "The way we can achieve this is by running a linear regression on the inventory levels with demand as explanatory variable. The regression model would be $s = \\beta d + \\varepsilon$ (without a constant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "X = unbiased_solution.state.demand\n",
    "y = unbiased_solution.decision.inventory\n",
    "reg = sm.OLS(y,X).fit()\n",
    "reg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *(s,s-1)* rule that could be derived from this would be $s = \\beta d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta = reg.params['demand']\n",
    "'basestock = %f*demand'%beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Comparison\n",
    "Now let's compare the performance of the linear decision rule with the optimal policy. For this, we are going to cut off the demand states of the last three stages to avoid end-of-horizon effects. We can then grab the coefficient from the regression model and implement a short simulation of the sequential decision process under the linear decision rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groups = solution.query('stage < 9').state.demand.groupby_series\n",
    "rewards = []\n",
    "for i in range(groups.ngroups):\n",
    "    inventory = initial_inventory\n",
    "    reward = 0.\n",
    "    for demand in groups.get_group(i).values:\n",
    "        sell = min(demand,inventory)\n",
    "        buy = max(0,beta*demand - (inventory-sell))\n",
    "        reward += sales_price*sell - buying_price*buy - inventory_cost*inventory\n",
    "        inventory = inventory + buy - sell\n",
    "    rewards.append(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can compare the total reward distribution of the linear decision rule with the total reward distribution for the same stages under the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rewards1 = pd.DataFrame(rewards, columns=['linear'])\n",
    "rewards2 = pd.DataFrame(solution.query('stage < 9').rewards.groupby_series.sum().values, columns=['stopro'])\n",
    "f, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "rewards1.plot(kind='kde', ax=ax, color='orange', linewidth=2)\n",
    "rewards2.plot(kind='kde', ax=ax, color='darkblue', linewidth=2, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comparison shows that the expected total rewards under each policy are not significantly different. This means that the simple linear policy provides a near-optimal rule of thumb that could be used by a practitioner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
